{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import re\n",
    "\n",
    "# --- Input PDF file ---\n",
    "pdf_path = \"127Riverside-Drive-DHCR.pdf\"\n",
    "\n",
    "# --- Derive folder name from PDF ---\n",
    "base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "output_dir = os.path.join(os.path.dirname(pdf_path), base_name)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Convert PDF pages to PNGs ---\n",
    "doc = fitz.open(pdf_path)\n",
    "for page_num in range(len(doc)):\n",
    "    pix = doc[page_num].get_pixmap(dpi=300)\n",
    "    output_path = os.path.join(output_dir, f\"page_{page_num+1}.png\")\n",
    "    pix.save(output_path)\n",
    "    print(f\"Saved: {output_path}\")\n",
    "\n",
    "print(f\"\\nâœ… All pages saved inside: {output_dir}\")\n",
    "\n",
    "# --- OCR part with natural sorting ---\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"Sort filenames containing numbers in human order.\"\"\"\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('(\\d+)', s)]\n",
    "\n",
    "# Gather all PNGs\n",
    "png_files = [f for f in os.listdir(output_dir) if f.endswith(\".png\")]\n",
    "png_files = sorted(png_files, key=natural_sort_key)  # <- apply natural sort\n",
    "\n",
    "# Combined OCR text\n",
    "all_text = \"\"\n",
    "txt_output_path = os.path.join(output_dir, f\"{base_name}.txt\")\n",
    "\n",
    "for filename in png_files:\n",
    "    image_path = os.path.join(output_dir, filename)\n",
    "    text = pytesseract.image_to_string(Image.open(image_path), lang=\"eng\")\n",
    "    all_text += f\"\\n\\n--- {filename} ---\\n\\n{text}\"\n",
    "\n",
    "# Save combined text\n",
    "with open(txt_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(all_text)\n",
    "\n",
    "print(f\"\\nâœ… OCR complete. Combined text saved at: {txt_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading text files from a folder\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Core LlamaIndex utilities\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "# Milvus vector store\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "# Ollama LLaMA 3.1 local model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# -------------------------------\n",
    "# 1ï¸âƒ£ Paths\n",
    "# -------------------------------\n",
    "\n",
    "# Folder where your OCR text files are\n",
    "data_dir = \"127Riverside-Drive-DHCR\"\n",
    "\n",
    "# -------------------------------\n",
    "# 2ï¸âƒ£ Load documents\n",
    "# -------------------------------\n",
    "\n",
    "documents = SimpleDirectoryReader(data_dir).load_data()\n",
    "print(f\"Loaded {len(documents)} documents for indexing.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3ï¸âƒ£ Connect Milvus\n",
    "# -------------------------------\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    collection_name=\"nyc_rent_docs\",\n",
    "    host=\"localhost\",  # adjust if your Milvus host is different\n",
    "    port=\"19530\",      # default Milvus port\n",
    "    dim=4096,          # match your embedding model dimension (llama3.1:8b)\n",
    "    embedding_field=\"embedding\"  # explicitly specify the embedding field name\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 4ï¸âƒ£ Initialize Ollama LLaMA3.1\n",
    "# -------------------------------\n",
    "\n",
    "# Configure global settings for LLM and embedding model\n",
    "Settings.llm = Ollama(model=\"llama3.1:8b\")  # use the correct model name\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"llama3.1:8b\")  # use the correct model name\n",
    "\n",
    "# -------------------------------\n",
    "# 5ï¸âƒ£ Build LlamaIndex\n",
    "# -------------------------------\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    vector_store=vector_store\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# -------------------------------\n",
    "# 6ï¸âƒ£ Example queries\n",
    "# -------------------------------\n",
    "\n",
    "questions = [\n",
    "    \"What is the legal regulated rent for apartment 22?\",\n",
    "    \"Who is the tenant of apartment 3?\",\n",
    "    \"List all tenants with a legal regulated rent above $1000.\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    response = query_engine.query(q)\n",
    "    print(f\"\\nâ“ {q}\\nðŸ’¬ {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8171d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bizai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
